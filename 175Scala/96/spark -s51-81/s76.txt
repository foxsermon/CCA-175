Problem Scenario 76 : You have been given MySQL DB with following details. 
User=retail_dba 
password=cloudera 
Database=retail_db 
Table=retail_db.orders 
jdbc URL = jdbc:mysql://quickstart:3306/retail_db 
Columns ot order table : (order_id , order_date , order_customer_id,order_status) 
Please accomplish following activities. 
1. Copy "retail_db.orders" table to hdfs in a directory p91_orders. 
2. Once data is copied to hdfs, using pyspark calculate the number of order for each status. 
3. use all the following methods to calculate the number of order for each status. (You need to know all these functions and its behavior for real exam) 
- countByKey() 
- groupByKey() 
- reduceByKey() 
- aggregateByKey() 
- combineByKey() 
=========================================================================
Solution : 
Step 1 : Import Single table 
sqoop import –connect jdbc:mysql://quickstart:3306/retail_db –username=retail_dba --password=cloudera -table=orders --target-dir=p91_orders –m1 
Note : Please check you dont have space between before or after '=' sign. 
Sqoop uses the MapReduce framework to copy data from RDBMS to hdfs 
Step 2 : Read the data from one of the partition, created using above command. 
hadoop fs -cat p91_orders/part-m-00000 
Step 3 : countByKey 
#Number of orders by status 
allOrders = sc.textFile("p91_orders") 
#Generate key and value pairs (key is order status and vale as an empty string 
keyValue = allOrders.map(lambda line: (line.split(",”)[3] “”)) 
#Using countByKey, aggregate data based on status as a key 
output=keyValue.countByKey().items() 
for line in output : print(line) 
Step 4 : groupByKey 
#Generate key and value pairs (key is order status and vale as an one 
keyValue = allOrders.map(lambda line: (line.split)”,”)[3],1)) 
#Using countByKey, aggregate data based on status as a key 
output= keyValue.groupByKey().map(lambda kv: (kv[O], sum(kv[l]))) 
for line in output.collect() : print(line) 
Step 5 : reduceByKey 
#Generate key and value pairs (key is order status and vale as an one 
keyValue = allOrders.map(lambda line: (line.split(",”)[3],1)) 
#Using countByKey, aggregate data based on status as a key 
output= keyValue.reduceByKey(lambda a, b: a + b) 
for line in output.collect() : print(line) 

Step 6 : aggregateByKey 
#Generate key and value pairs (key is order status and vale as an one 

keyValue = allOrders.map(lambda line: (line.split(“,”)[3],line)) 
output=keyValue.aggregateByKey(0, lambda a, b: a+l, lambda a, b: a+b) 
for line in output.collect() : print(line) 
Step 7 : combineByKey 
#Generate key and value pairs (key is order status and vale as an one 
keyValue = allOrders.map(lambda line: (line.split(“,”)[3], line)) 
output=keyValue.combineByKey(lambda value: 1, lambda acc, value: acc+l, lambda acc, value: acc+value) 
for line in output.collect() : print(line) 
#Watch Spark Protessional Training provided by www.HadoopExam.com to understand more on each above functions. 
(These are very important functions for real exam)
