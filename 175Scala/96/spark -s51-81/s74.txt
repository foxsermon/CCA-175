Problem Scenario 74 : You have been given MySQL DB with following details. 
User=retail_dba 
password=cloudera 
database=retail_db 
table=retail_db.orders 
Table=retail_db.order_items 
jdbc URL = jdbc:mysql://quickstart:3306/retail_db 
Columns of order table : (order_id , order_date , order_customer_id, order_status) 

Columns of order_items table : (order_item_id , order_item_order_ 
id , order_item_product_id, order_item_quantity,order_item_subtotal,order_
item_product_price) 
Please accomplish following activities. 
1. Copy "retail_db.orders" and "retail_db.order_items" table to hdfs in respective directory p89_orders and p89_order_items . 
2. Join these data using order_id in Spark and Python 
3. Now fetch selected columns from joined data Orderld, Order date and amount collected on this order. 
4. Calculate total order placed for each date, and produced the output sorted by date. 
=====================================================================

Solution : 

Step 1 : Import Single table . 
sqoop import –connect jdbc:mysql://quickstart:3306/retail_db –username=retail_dba --password=cloudera -table=orders_items—target-dir=p89_orders –m1 
sqoop import –connect jdbc:mysql://quickstart:3306/retail_db --username=retail_dba --password=cloudera -table=order_items --target-dir=p89_order_items –m1 
Note : Please check you dont have space between before or after ‘=’ sign. 
Sqoop uses the MapReduce framework to copy data from RDBMS to hdfs 
Step 2 : Read the data from one of the partition, created using above command. 
hadoop fs -cat p89_orders/part-m-00000 
hadoop fs -cat p89_order_items/part-m-OOOOO 
Step 3 : Load these above two directory as RDD using Spark and Python (Open pyspark terminal and do following). 
orders = sc.textFile("p89_orders") 
orderltems = sc.textFile("p89_order_items") 
Step 4 : Convert RDD info key value as (order_id as a key and rest of the values as a value) 
#First value is order_id 
ordersKeyValue = orders.map(lambda line: (int(line.split(“,”)[0]), line)) 
#Second value as an Order id 
orderltemsKeyValue = orderltems.map(lambda line: (int(line.split(",”)[1]),line)) 
Step 5 : Join both the RDD using order_id 
joinedData = orderltemsKeyValue.join(ordersKeyValue) 
#print the joined data 
for line in joinedData.collect(): 
print(line) 
Format of joinedData as below. 
[Orderid, 'All columns from orderltemsKeyValue' , 'All columns from ordersKeyValue'] 
Step 6 : Now fetch selected values Orderld, Order date and amount collected on this order. 
revenuePerOrderPerDay = joinedData.map(lambda row: (row[O],row[1][1].split(“,”)[1],float(row[1][0].split(“,”)[4]))) 
#print the result 
for line in revenuePerOrderPerDay.collect(): 
print(line) 
Step 7 : Select distinct order ids for each date. 
#distinct(date,order_id) 
distinctOrdersDate = joinedData.map(lambda row:row[1][1].split(“,”)[1]+”,”+str(row[0])).distinct() 
for line in distinctOrdersDate.collect(): 

print(line) 
Step 8 : Similar to word count , generate (date, 1) record for each row. 
newLineTuple = distinctOrdersDate.map(lambda line: (line.split(",”)[0],1)) 
Step 9 : Do the count for each key(date), to get total order per date. 
totalOrdersPerDate = newLineTuple.reduceByKey(lambda a, b: a + b) 
#print results 
for line in totalOrdersPerDate.collect(): 
print(line) 
Step 10 : Sort the results by date 
sortedData=totalOrdersPerDate.sortByKey().collect() 
#print results 
for line in sortedData: 
print(line) 
