Problem Scenario 27 : You need to implement near real time solutions for collecting information when submitted in file with below information. 
You have been given below directory location (if not available than create it) /tmp/spooldir . You have a finacial subscription for getting stock prices from Bloomberg as well as
Reuters and using ftp you download every hour new files from their respective ftp site in directories /tmp/spooldir/bb and /tmp/spooldir respectively. 
As soon as tile committed in this directory that needs to be available in hdfs in /tmp/flume/finance location in a single directory. 
Write a flume configuration file named flume7.conf and use it to load data in hdfs with following additional properties . 
1. Spool /tmp/spooldir/bb and /tmp/spooldir/dr 
2. File prefix in hdfs sholuld be events 
3. File suffix should be .log 
4. If file is not commited and in use than it should have_as prefix
5. Data should be written as text to hdfs.
===================================================================== 
Solution : 

Step 1 : Create directory 
mkdir /tmp/spooldir/bb
mkdir /tmp/spooldir/dr 
step 2 : Create flume configuration file, with below configuration for source, sink and channel and save it in flume7.conf. 
agent1 .sources = source1 source2 
agent1 .sinks = sink1 
agent1 .channels = channel1
agent1 .sources.source1 .channels = channel1 
agent1 .sources.source2.channels = channel1
agent1 .sinks.sink1 .channel = channel1 
agent1 .sources.source1 .type = spooldir 
agent1 .sources.source1 .spoolDir = /tmp/spooldir/bb 
agent1 .sources.source2.type = spooldir 
agent1. sources. source2.spoolDir = /tmp/spooldir/dr 
agent1 .sinks.sink1 .type = hdfs 
agent1 .sinks.sink1 .hdfs.path = /tmp/flume/finance 
agent1 .sinks.sink1.hdfs.filePrefix = events 
agent1 .sinks.sink1 .hdfs.fileSuffix = .log 
agent1 .sinks.sink1 .hdfs.inuseprefix =_ 
agent1 .sinks.sink1 .hdfs.fileType = DataStream 

agent1 .channels.channel1 .type = file 

Step 4 : Run below command which will use this configuration file and append data in hdfs. 
Start flume service : 
flume-ng agent --conf /home/cloudera/flumeconf --conf-file /home/cloudera/flumeconf/flume7.conf –name agent1 
Step 5 : Open another terminal and create a file in /tmp/spooldir/ 
echo “IBM ,100.20160104”>> “/tmp/spooldir/bb/.bb.txt 
echo”IBM,103.2016010” >> /tmp/spooldir/bb/.bb.txt 
mv /tmp/spooldir/bb/.bb.txt /tmp/spooldir/bb/bb.txt 
After few mins 
echo “IBM,100.2,20160104>> /tmp/spooldir/dr/.dr.txt 
echo “IBM,103.1,20160105”>> /tmp/spooldir/dr/.dr.txt 
mv /tmp/spooldir/dr/.dr.txt /tmp/spooldir/dr/dr.txt 
